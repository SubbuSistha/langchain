# LangChain Prompt Templates

## What is a Prompt Template?

A **Prompt Template** in LangChain is a **structured and reusable way to create prompts** for LLMs.

> Instead of hardcoding strings, you define a template with variables.

---

## ðŸ§  Why Use Prompt Templates?

Without template:

```python
topic = "AI"
llm.invoke("Explain " + topic)
```

ðŸ‘‰ Hard to maintain
ðŸ‘‰ Not reusable

---

With template:

```python
"Explain {topic}"
```

ðŸ‘‰ Dynamic
ðŸ‘‰ Clean
ðŸ‘‰ Reusable

---

## ðŸ”¥ PromptTemplate (Text-based)

Used for simple text prompts.

### Example

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    "Explain {topic} in simple terms"
)

result = prompt.format(topic="AI")
print(result)
```

---

## Output

```
Explain AI in simple terms
```

---

## ðŸ§  Similar to f-string

### Python f-string

```python
topic = "AI"
text = f"Explain {topic}"
```

### PromptTemplate

```python
prompt.format(topic="AI")
```

---

## ðŸ”¥ Advantages over f-string

| Feature                 | f-string | PromptTemplate |
| ----------------------- | -------- | -------------- |
| Reusable                | âŒ        | âœ…              |
| Validation              | âŒ        | âœ…              |
| Partial variables       | âŒ        | âœ…              |
| Integration with chains | âŒ        | âœ…              |

---

## ðŸ”¥ Multiple Variables

```python
prompt = PromptTemplate.from_template(
    "Explain {topic} in {level} level"
)

prompt.format(topic="AI", level="beginner")
```

---

## ðŸ”¥ Partial Variables

Fix some variables:

```python
prompt = PromptTemplate.from_template(
    "Explain {topic} in {level} level"
)

partial_prompt = prompt.partial(level="simple")

partial_prompt.format(topic="AI")
```

---

## ðŸ§  Output

```
Explain AI in simple level
```

---

## ðŸ”¥ ChatPromptTemplate (Recommended)

Used for chat-based models.

Supports roles:

* system
* human
* ai

---

## Example (Tuple style)

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("human", "Explain {topic}")
])

messages = prompt.invoke({"topic": "AI"})
```

---

## Example (Message objects)

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="You are a helpful assistant"),
    HumanMessage(content="Explain {topic}")
])
```

---

## ðŸ§  Output

Returns structured messages:

```
SystemMessage(...)
HumanMessage(...)
```

---

## ðŸ”¥ Using with LLM

```python
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")

chain = prompt | llm

response = chain.invoke({"topic": "LangChain"})

print(response.content)
```

---

## ðŸ§  Flow

```
Input â†’ PromptTemplate â†’ LLM â†’ Output
```

---

## ðŸ”¥ With Output Parser

```python
from langchain_core.output_parsers import StrOutputParser

chain = prompt | llm | StrOutputParser()

result = chain.invoke({"topic": "AI"})
print(result)
```

---

## ðŸ§  Key Benefits

* Reusable prompts
* Cleaner code
* Dynamic variables
* Easy integration with chains
* Works with chat models

---

## âš ï¸ Common Mistakes

| Mistake               | Fix                    |
| --------------------- | ---------------------- |
| Hardcoding prompt     | Use template           |
| Missing variables     | Pass all inputs        |
| Using string concat   | Avoid                  |
| Not using chat prompt | Use ChatPromptTemplate |

---

## ðŸ§  Best Practices

* Use **PromptTemplate** for simple text
* Use **ChatPromptTemplate** for chat models
* Use **partial()** for fixed variables
* Keep prompts clear and structured

---

## âœ… Final Summary

* PromptTemplate = dynamic text prompt
* ChatPromptTemplate = structured chat prompt
* Supports variables and reuse
* Works with chains and agents

---

## One-Line Definition

> **LangChain Prompt Template is a reusable, structured way to generate dynamic inputs for LLMs.**

---
